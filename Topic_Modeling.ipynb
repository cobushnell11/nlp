{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYOHte7NlGJR"
   },
   "source": [
    "# **In-Class Assignment: Topic Modeling**\n",
    "\n",
    "## *IS 5150*\n",
    "## Name: Cailean\n",
    "\n",
    "Key-phrase extraction relatively simple and easy to implement, but doesn't always yield the most useful information in regards to determining a document's overall theme/topic. Topic modeling is a more sophisticated approach to extracting topics/themes from a corpus, and there are several unsupervised approaches that can accomplish this text mining task:\n",
    "\n",
    "*   Latent Semantic Indexing\n",
    "*   Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "In this notebook, I implement the above methods on a corpus of research papers from the NeurIPS conference to try and extract meaningful topic labels for these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giM7h9Q9r4wN",
    "outputId": "dcbca2ec-5335-4454-c9ab-506bd3b247f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# load all dependencies\n",
    "\n",
    "import nltk\n",
    "nltk.download(['stopwords', 'wordnet', 'omw-1.4'])  #stopwords, wordnet, omw-1.4\n",
    "import gensim\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS2-VuCrr4gS"
   },
   "source": [
    "## **1) Data Retrieval**\n",
    "\n",
    "Download the dataset directly using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_TCas0xk2qc",
    "outputId": "7800327c-1954-48d3-dea5-f9fd53001edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-03 01:05:56--  https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
      "Resolving cs.nyu.edu (cs.nyu.edu)... 216.165.22.203\n",
      "Connecting to cs.nyu.edu (cs.nyu.edu)|216.165.22.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12851423 (12M) [application/x-gzip]\n",
      "Saving to: ‘nips12raw_str602.tgz’\n",
      "\n",
      "nips12raw_str602.tg 100%[===================>]  12.26M  9.11MB/s    in 1.3s    \n",
      "\n",
      "2022-11-03 01:05:57 (9.11 MB/s) - ‘nips12raw_str602.tgz’ saved [12851423/12851423]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSZTp9edn0ym"
   },
   "source": [
    "#### **A) Data Extraction**\n",
    "\n",
    "Uncompress this tgz file and extract the different text files within each of the subfolders, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkvnO5VBn5GZ"
   },
   "outputs": [],
   "source": [
    "!tar -xzf nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5lhQiF7oAny"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/content/nipstxt'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHORRD_AoLO8",
    "outputId": "95d07c3a-bd70-43dc-e09d-91c4bc5c404e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "                                                                                                                              # Read all texts into a list\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + '/' + folder )\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + '/' + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzPAZtyyqKZg"
   },
   "source": [
    "**If using Jupyter notebooks/labs using this instead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrU9i_XvqOR_"
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "# # Read all texts into a list.\n",
    "# papers = []\n",
    "# for folder in folders:\n",
    "#     file_names = os.listdir(DATA_PATH + folder)\n",
    "#     for file_name in file_names:\n",
    "#         with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "#             data = f.read()\n",
    "#         papers.append(data)\n",
    "# len(papers)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VlhVBNxBoOSg",
    "outputId": "6e839293-e337-45c7-a6c7-5bd5385c2b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 \n",
      "NEW HARDWARE FOR MASSIVE NEURAL NETWORKS \n",
      "D. D. Coon and A. G. U. Perera \n",
      "Applied Technology Laboratory \n",
      "University of Pittsburgh \n",
      "Pittsburgh, PA 15260. \n",
      "ABSTRACT \n",
      "Transient phenomena associated with forward biased silicon p+ - n - n + struc- \n",
      "tures at 4.2K show remarkable similarities with biological neurons. The devices play \n",
      "a role similar to the two-terminal switching elements in Hodgkin-Huxley equivalent \n",
      "circuit diagrams. The devices provide simpler and more realistic neuron emulation \n",
      "than transistors or op-amps. They have such low power and current requirements \n",
      "that they could be used in massive neural networks. Some observed properties of \n",
      "simple circuits containing the devices include action potentials, refractory periods, \n",
      "threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic \n",
      "weights, temporal integration, memory, network connectivity modification based on \n",
      "experience, pacemaker activity, firing thresholds, coupling to sensors with grade\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOJaEvsTrXPp"
   },
   "source": [
    "## **2) Basic Text Preprocessing**\n",
    "\n",
    "I create a separate normalize_corpus function here, because I don't need to do all the steps we normally would. This is going to save us some time in its execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQty16iyrjYO",
    "outputId": "ae1c2b43-5eef-4442-ae68-78bbe8b17854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "CPU times: user 27.5 s, sys: 180 ms, total: 27.7 s\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower() # set paper to lowercase\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)] # tokenize paper and strip extra whitespaces\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]  # lemmatize tokens\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1] # remove tokens shorter than 1 character or shorter\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words] # remove stopwords\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "\n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80oqa0q_so2E",
    "outputId": "122399c7-bf69-4a43-c597-3d58fb27c114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'hardware', 'massive', 'neural', 'network', 'coon', 'perera', 'applied', 'technology', 'laboratory', 'university', 'pittsburgh', 'pittsburgh', 'pa', 'abstract', 'transient', 'phenomenon', 'associated', 'forward', 'biased', 'silicon', 'struc', 'tures', '2k', 'show', 'remarkable', 'similarity', 'biological', 'neuron', 'device', 'play', 'role', 'similar', 'two', 'terminal', 'switching', 'element', 'hodgkin', 'huxley', 'equivalent', 'circuit', 'diagram', 'device', 'provide', 'simpler', 'realistic', 'neuron', 'emulation', 'transistor', 'op', 'amp', 'low', 'power', 'current', 'requirement', 'could', 'used', 'massive', 'neural', 'network', 'observed', 'property', 'simple', 'circuit', 'containing', 'device', 'include', 'action', 'potential', 'refractory', 'period', 'threshold', 'behavior', 'excitation', 'inhibition', 'summation', 'synaptic', 'input', 'synaptic', 'weight', 'temporal', 'integration', 'memory', 'network', 'connectivity', 'modification', 'based', 'experience', 'pacemaker', 'activity', 'firing', 'threshold', 'coupling', 'sensor', 'graded', 'sig', 'nal', 'output', 'dependence', 'firing']\n"
     ]
    }
   ],
   "source": [
    "print(norm_papers[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHmzLnLgw27M"
   },
   "source": [
    "## 3) **Feature Engineering**\n",
    "\n",
    "Before performing any sort of vectorization, I narrow down the pool of words to more common n-grams; specifically bigrams that have occurred in the text at least 20 times. This is going to cut down on the number of words to vectorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYj9tAklxSRN"
   },
   "outputs": [],
   "source": [
    "# ~ number of words in corpus\n",
    "len(set(norm_papers[2]))*1740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbjAjkiEyGZr",
    "outputId": "fa35da88-4eb4-4e2a-8b9f-cba62356b6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'hardware', 'massive', 'neural_network', 'coon', 'perera', 'applied', 'technology', 'laboratory', 'university_pittsburgh', 'pittsburgh_pa', 'abstract', 'transient', 'phenomenon', 'associated', 'forward', 'biased', 'silicon', 'struc', 'tures', '2k', 'show', 'remarkable', 'similarity', 'biological', 'neuron', 'device', 'play_role', 'similar', 'two', 'terminal', 'switching', 'element', 'hodgkin_huxley', 'equivalent', 'circuit_diagram', 'device', 'provide', 'simpler', 'realistic', 'neuron', 'emulation', 'transistor', 'op', 'amp', 'low_power', 'current', 'requirement', 'could', 'used']\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_')                     # higher threshold fewer phrases.\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "print(bigram_model[norm_papers[0]][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j-Zj5eyyasp"
   },
   "source": [
    "> #### **A) Extract bigrams from full corpus to create a condensed vocab set of high frequency phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuZh08G6ynKU",
    "outputId": "494b911f-bef9-46ca-8689-02dee504173e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings: [(0, '100fa'), (1, '10ma'), (2, '2k'), (3, '2o3'), (4, '2o6'), (5, '80er10667'), (6, 'ability'), (7, 'abruptly'), (8, 'abstract'), (9, 'ac'), (10, 'acad_sci'), (11, 'accomplished'), (12, 'accomplishes'), (13, 'accumulation'), (14, 'acheivable')]\n",
      "Total Vocabulary Size: 78892\n"
     ]
    }
   ],
   "source": [
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)                                                   # Create a dictionary representation of the documents\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])                                       # print out sample of dictionary\n",
    "print('Total Vocabulary Size:', len(dictionary))                                                              # print total vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRIMxjdczZQ8"
   },
   "source": [
    "> #### **B) Prune the vocab set by removing words that occur in fewer than 20 documents or more than 60% of documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xN9Tyjshzmd7"
   },
   "source": [
    "**What is the logic behind this step?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDYOem-wITMQ"
   },
   "source": [
    "We don't need to include very frequent or very infrequent words/bigrams; those typically don't add much value to the meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYM9JtFdzreH",
    "outputId": "af2ec3df-28af-4656-f7c4-58cc600608be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 7756\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=20, no_above=0.60)\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isv8rZq-bwgz"
   },
   "source": [
    "> #### **C) Bag of Words Vectorization**\n",
    "\n",
    "Great! Now with a much more reasonable vocab size, I won't have so many dimensions. Let's create the BOW corpus from the dictionary of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqOi5rH3cSwp",
    "outputId": "25010868-06bb-4d5b-dc40-2f4c794bf910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (4, 1), (9, 1), (14, 11), (15, 1), (17, 1), (18, 2), (21, 1), (26, 1), (27, 1), (28, 3), (30, 4), (31, 1), (35, 1), (41, 10), (42, 7), (45, 1), (46, 2), (50, 1), (52, 1), (54, 2), (55, 2), (60, 4), (64, 1), (73, 1), (82, 6), (84, 1), (86, 1), (93, 1), (94, 3), (95, 5), (96, 3), (98, 5), (100, 1), (102, 3), (107, 4), (109, 1), (110, 3), (113, 1), (121, 7), (122, 2), (126, 1), (131, 1), (134, 2), (136, 3), (146, 1), (150, 1), (153, 1), (154, 1), (155, 1)]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print(bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3OnHoEqcYpd"
   },
   "source": [
    "**What does each tuple represent here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPMinIQ_cbPn"
   },
   "source": [
    "A word index and its frequency in document 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhDkAkoccm4Q",
    "outputId": "aefc39c9-47ea-4dc4-a649-ab4cb25809cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('acad_sci', 1), ('according', 1), ('addition', 1), ('american_institute', 1), ('application', 2), ('approximation', 2), ('assume', 2), ('assumption', 5), ('available', 1), ('average', 4), ('averaged', 1), ('best', 3), ('binocular', 1), ('cannot', 1), ('center', 2), ('change', 6), ('class', 1), ('compared', 1), ('complex', 1), ('component', 3), ('computation', 5), ('computed', 4), ('computing', 3), ('consider', 1), ('consistent', 1), ('constant', 5), ('constraint', 8), ('contain', 1), ('corresponding', 4), ('could', 1), ('critical', 1), ('dark', 2), ('define', 1), ('density', 1), ('difference', 1), ('discussed', 1), ('due', 2), ('ed', 3), ('effect', 3), ('effective', 1), ('either', 1), ('equation', 2), ('et_al', 1), ('evolution', 1), ('experiment', 1), ('explicit', 1), ('eye', 1), ('fact', 1), ('fairly', 1), ('find', 3)]\n"
     ]
    }
   ],
   "source": [
    "print([(dictionary[idx], freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKW4g_CadM64"
   },
   "source": [
    "## **4) Topic Modeling with Latent Semantic Indexing (LSI)**\n",
    "\n",
    "I use latent semantic indexing first to generate a topic model. I set the total number of topics to 10 and then examine which words are most influential for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4J6zIxVdQrU",
    "outputId": "493d51ec-3b1d-4488-c445-14c88fa25e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 55s, sys: 1min 50s, total: 6min 46s\n",
      "Wall time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOTAL_TOPICS = 10\n",
    "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS,\n",
    "                                 onepass=True, chunksize=1740, power_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_TTfsRihQ8P"
   },
   "source": [
    "> #### **A) Print topics (top 20 words per 10 topics)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkKo5ZjtdgQs",
    "outputId": "917033ad-f2e8-4a45-fb47-f255081e9e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.215*\"unit\" + 0.212*\"state\" + 0.187*\"training\" + 0.177*\"neuron\" + 0.162*\"pattern\" + 0.145*\"image\" + 0.140*\"vector\" + 0.125*\"feature\" + 0.122*\"cell\" + 0.110*\"layer\" + 0.101*\"task\" + 0.097*\"class\" + 0.091*\"probability\" + 0.089*\"signal\" + 0.087*\"step\" + 0.086*\"response\" + 0.085*\"representation\" + 0.083*\"noise\" + 0.082*\"rule\" + 0.081*\"distribution\"\n",
      "\n",
      "Topic #2:\n",
      "0.487*\"neuron\" + 0.396*\"cell\" + -0.257*\"state\" + 0.191*\"response\" + -0.187*\"training\" + 0.170*\"stimulus\" + 0.117*\"activity\" + -0.109*\"class\" + 0.099*\"spike\" + 0.097*\"pattern\" + 0.096*\"circuit\" + 0.096*\"synaptic\" + -0.095*\"vector\" + 0.090*\"signal\" + 0.090*\"firing\" + 0.088*\"visual\" + -0.084*\"classifier\" + -0.083*\"action\" + -0.078*\"word\" + 0.078*\"cortical\"\n",
      "\n",
      "Topic #3:\n",
      "-0.627*\"state\" + 0.395*\"image\" + -0.219*\"neuron\" + 0.209*\"feature\" + -0.188*\"action\" + 0.137*\"unit\" + 0.131*\"object\" + -0.130*\"control\" + 0.129*\"training\" + -0.109*\"policy\" + 0.103*\"classifier\" + 0.090*\"class\" + -0.081*\"step\" + -0.081*\"dynamic\" + 0.080*\"classification\" + 0.078*\"layer\" + 0.076*\"recognition\" + -0.074*\"reinforcement_learning\" + 0.069*\"representation\" + 0.068*\"pattern\"\n",
      "\n",
      "Topic #4:\n",
      "0.686*\"unit\" + -0.433*\"image\" + 0.182*\"pattern\" + 0.131*\"layer\" + 0.123*\"hidden_unit\" + 0.121*\"net\" + 0.114*\"training\" + -0.112*\"feature\" + 0.109*\"activation\" + 0.107*\"rule\" + -0.097*\"neuron\" + 0.078*\"word\" + -0.070*\"pixel\" + 0.070*\"connection\" + -0.067*\"object\" + -0.065*\"state\" + -0.060*\"distribution\" + -0.059*\"face\" + 0.057*\"architecture\" + -0.055*\"estimate\"\n",
      "\n",
      "Topic #5:\n",
      "-0.428*\"image\" + -0.348*\"state\" + 0.266*\"neuron\" + -0.264*\"unit\" + 0.181*\"training\" + 0.174*\"class\" + -0.168*\"object\" + 0.167*\"classifier\" + -0.147*\"action\" + -0.122*\"visual\" + 0.117*\"vector\" + 0.115*\"node\" + 0.105*\"distribution\" + -0.103*\"motion\" + -0.099*\"feature\" + 0.097*\"classification\" + -0.097*\"control\" + -0.095*\"task\" + -0.087*\"cell\" + -0.083*\"representation\"\n",
      "\n",
      "Topic #6:\n",
      "0.660*\"cell\" + -0.508*\"neuron\" + -0.213*\"image\" + -0.103*\"chip\" + -0.097*\"unit\" + 0.093*\"response\" + -0.090*\"object\" + 0.083*\"rat\" + 0.076*\"distribution\" + -0.070*\"circuit\" + 0.069*\"probability\" + 0.064*\"stimulus\" + -0.061*\"memory\" + -0.058*\"analog\" + -0.058*\"activation\" + 0.055*\"class\" + -0.053*\"bit\" + -0.052*\"net\" + 0.051*\"cortical\" + 0.050*\"firing\"\n",
      "\n",
      "Topic #7:\n",
      "-0.353*\"word\" + 0.281*\"unit\" + -0.272*\"training\" + -0.257*\"classifier\" + -0.177*\"recognition\" + 0.159*\"distribution\" + -0.152*\"feature\" + -0.144*\"state\" + -0.142*\"pattern\" + 0.141*\"vector\" + -0.128*\"cell\" + -0.128*\"task\" + 0.122*\"approximation\" + 0.121*\"variable\" + 0.110*\"equation\" + -0.107*\"classification\" + 0.106*\"noise\" + -0.103*\"class\" + 0.101*\"matrix\" + -0.098*\"neuron\"\n",
      "\n",
      "Topic #8:\n",
      "0.303*\"pattern\" + -0.243*\"signal\" + -0.236*\"control\" + -0.202*\"training\" + 0.181*\"rule\" + 0.178*\"state\" + -0.167*\"noise\" + 0.166*\"class\" + -0.162*\"word\" + 0.155*\"cell\" + 0.154*\"feature\" + -0.147*\"motion\" + -0.140*\"task\" + 0.127*\"node\" + 0.124*\"neuron\" + -0.116*\"target\" + -0.114*\"circuit\" + 0.114*\"probability\" + 0.110*\"classifier\" + 0.109*\"image\"\n",
      "\n",
      "Topic #9:\n",
      "0.472*\"node\" + 0.254*\"circuit\" + -0.214*\"word\" + 0.201*\"chip\" + -0.190*\"neuron\" + -0.172*\"stimulus\" + 0.160*\"classifier\" + 0.152*\"current\" + -0.147*\"feature\" + 0.146*\"voltage\" + -0.145*\"distribution\" + 0.141*\"control\" + 0.124*\"rule\" + 0.110*\"layer\" + 0.105*\"analog\" + 0.091*\"tree\" + -0.084*\"response\" + -0.080*\"state\" + -0.079*\"probability\" + -0.079*\"estimate\"\n",
      "\n",
      "Topic #10:\n",
      "0.518*\"word\" + -0.254*\"training\" + 0.236*\"vector\" + -0.222*\"task\" + -0.194*\"pattern\" + -0.156*\"classifier\" + 0.149*\"node\" + 0.146*\"recognition\" + -0.139*\"control\" + 0.138*\"sequence\" + -0.126*\"rule\" + 0.125*\"circuit\" + 0.123*\"cell\" + -0.113*\"action\" + -0.105*\"neuron\" + 0.094*\"hmm\" + 0.093*\"character\" + 0.088*\"chip\" + 0.088*\"matrix\" + 0.085*\"structure\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lsi_bow.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKBuDXSLf9QP"
   },
   "source": [
    "**Like a correlation coefficient, the larger the associated weight, the stronger the influence of a word within a topic. But what does it mean for a topic to have words with positive and negative weights?**\n",
    "\n",
    "Two main ideas being captured within one topic, when we have both positive and negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIHed9G7fp6o",
    "outputId": "a0fa8af8-c0d0-45c3-b59e-c60efe2e32f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.215), ('state', 0.212), ('training', 0.187), ('neuron', 0.177), ('pattern', 0.162), ('image', 0.145), ('vector', 0.14), ('feature', 0.125), ('cell', 0.122), ('layer', 0.11), ('task', 0.101), ('class', 0.097), ('probability', 0.091), ('signal', 0.089), ('step', 0.087), ('response', 0.086), ('representation', 0.085), ('noise', 0.083), ('rule', 0.082), ('distribution', 0.081)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.487), ('cell', 0.396), ('response', 0.191), ('stimulus', 0.17), ('activity', 0.117), ('spike', 0.099), ('pattern', 0.097), ('circuit', 0.096), ('synaptic', 0.096), ('signal', 0.09), ('firing', 0.09), ('visual', 0.088), ('cortical', 0.078)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.257), ('training', -0.187), ('class', -0.109), ('vector', -0.095), ('classifier', -0.084), ('action', -0.083), ('word', -0.078)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('image', 0.395), ('feature', 0.209), ('unit', 0.137), ('object', 0.131), ('training', 0.129), ('classifier', 0.103), ('class', 0.09), ('classification', 0.08), ('layer', 0.078), ('recognition', 0.076), ('representation', 0.069), ('pattern', 0.068)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.627), ('neuron', -0.219), ('action', -0.188), ('control', -0.13), ('policy', -0.109), ('step', -0.081), ('dynamic', -0.081), ('reinforcement_learning', -0.074)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.686), ('pattern', 0.182), ('layer', 0.131), ('hidden_unit', 0.123), ('net', 0.121), ('training', 0.114), ('activation', 0.109), ('rule', 0.107), ('word', 0.078), ('connection', 0.07), ('architecture', 0.057)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.433), ('feature', -0.112), ('neuron', -0.097), ('pixel', -0.07), ('object', -0.067), ('state', -0.065), ('distribution', -0.06), ('face', -0.059), ('estimate', -0.055)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.266), ('training', 0.181), ('class', 0.174), ('classifier', 0.167), ('vector', 0.117), ('node', 0.115), ('distribution', 0.105), ('classification', 0.097)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.428), ('state', -0.348), ('unit', -0.264), ('object', -0.168), ('action', -0.147), ('visual', -0.122), ('motion', -0.103), ('feature', -0.099), ('control', -0.097), ('task', -0.095), ('cell', -0.087), ('representation', -0.083)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('cell', 0.66), ('response', 0.093), ('rat', 0.083), ('distribution', 0.076), ('probability', 0.069), ('stimulus', 0.064), ('class', 0.055), ('cortical', 0.051), ('firing', 0.05)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -0.508), ('image', -0.213), ('chip', -0.103), ('unit', -0.097), ('object', -0.09), ('circuit', -0.07), ('memory', -0.061), ('analog', -0.058), ('activation', -0.058), ('bit', -0.053), ('net', -0.052)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.281), ('distribution', 0.159), ('vector', 0.141), ('approximation', 0.122), ('variable', 0.121), ('equation', 0.11), ('noise', 0.106), ('matrix', 0.101)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -0.353), ('training', -0.272), ('classifier', -0.257), ('recognition', -0.177), ('feature', -0.152), ('state', -0.144), ('pattern', -0.142), ('cell', -0.128), ('task', -0.128), ('classification', -0.107), ('class', -0.103), ('neuron', -0.098)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('pattern', 0.303), ('rule', 0.181), ('state', 0.178), ('class', 0.166), ('cell', 0.155), ('feature', 0.154), ('node', 0.127), ('neuron', 0.124), ('probability', 0.114), ('classifier', 0.11), ('image', 0.109)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('signal', -0.243), ('control', -0.236), ('training', -0.202), ('noise', -0.167), ('word', -0.162), ('motion', -0.147), ('task', -0.14), ('target', -0.116), ('circuit', -0.114)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('node', 0.472), ('circuit', 0.254), ('chip', 0.201), ('classifier', 0.16), ('current', 0.152), ('voltage', 0.146), ('control', 0.141), ('rule', 0.124), ('layer', 0.11), ('analog', 0.105), ('tree', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -0.214), ('neuron', -0.19), ('stimulus', -0.172), ('feature', -0.147), ('distribution', -0.145), ('response', -0.084), ('state', -0.08), ('probability', -0.079), ('estimate', -0.079)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('word', 0.518), ('vector', 0.236), ('node', 0.149), ('recognition', 0.146), ('sequence', 0.138), ('circuit', 0.125), ('cell', 0.123), ('hmm', 0.094), ('character', 0.093), ('chip', 0.088), ('matrix', 0.088), ('structure', 0.085)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -0.254), ('task', -0.222), ('pattern', -0.194), ('classifier', -0.156), ('control', -0.139), ('rule', -0.126), ('action', -0.113), ('neuron', -0.105)]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sua0a-vYgS1f"
   },
   "source": [
    "**Spend some time examining the different words amongst topics 1-10 and their 2 subtopics; what are some themes/possible topics that emerge?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8hs0uJpgfAO"
   },
   "source": [
    "Can indicate two distrinct sub-topics (two directions) within one topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQi3GP2zhEuC"
   },
   "source": [
    "> #### **B) Apply SVD to decompose our term-document matrix into term-topic, topic-topic, and topic-document matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YC7J-spAh4Mg",
    "outputId": "d7950c4a-be6b-4305-92e9-4557a1d7e4b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/matutils.py:502: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  result = np.column_stack(sparse2full(doc, num_terms) for doc in corpus)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7756, 10), (10,), (10, 1740))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_topic = lsi_bow.projection.u\n",
    "singular_values = lsi_bow.projection.s\n",
    "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBXuuY_-iQ-1"
   },
   "source": [
    "> #### **C) Transpose to Document Topic Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "Q73z0V0GiVPt",
    "outputId": "dfee6487-1af2-4799-dd10-15f777e0772d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-72a5665c-cbf1-4497-a7ea-c4cbd040fba4\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.017</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.029</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.023</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.027</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.037</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72a5665c-cbf1-4497-a7ea-c4cbd040fba4')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-72a5665c-cbf1-4497-a7ea-c4cbd040fba4 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-72a5665c-cbf1-4497-a7ea-c4cbd040fba4');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       T1     T2     T3     T4     T5     T6     T7     T8     T9    T10\n",
       "0   0.023  0.028 -0.018 -0.015  0.006 -0.032 -0.009 -0.011  0.049  0.021\n",
       "1   0.041 -0.014 -0.017  0.032 -0.018 -0.003  0.062 -0.005 -0.001  0.021\n",
       "2   0.016  0.017 -0.013 -0.008  0.024 -0.028  0.000  0.019 -0.008 -0.006\n",
       "3   0.035 -0.002 -0.017  0.008  0.016 -0.017  0.032  0.022  0.050  0.029\n",
       "4   0.022 -0.004 -0.033 -0.008 -0.029  0.008 -0.009  0.007 -0.016  0.003\n",
       "5   0.017  0.020 -0.007 -0.013  0.003  0.030  0.009 -0.009  0.035  0.017\n",
       "6   0.020  0.034 -0.019 -0.003  0.017 -0.042 -0.012  0.014  0.002 -0.012\n",
       "7   0.045 -0.007 -0.002  0.030  0.024  0.007 -0.027  0.009  0.019 -0.049\n",
       "8   0.029  0.052  0.001  0.021  0.003  0.055 -0.041  0.052  0.019 -0.013\n",
       "9   0.017 -0.002 -0.009 -0.011  0.017 -0.002  0.027  0.009 -0.002  0.005\n",
       "10  0.027 -0.007  0.027 -0.041 -0.016 -0.008  0.036 -0.006  0.005  0.006\n",
       "11  0.023  0.003  0.006 -0.031 -0.016 -0.004  0.028 -0.021  0.046  0.021\n",
       "12  0.027  0.055 -0.018 -0.004 -0.006  0.083 -0.011  0.039 -0.004  0.014\n",
       "13  0.037  0.007 -0.001  0.007  0.034 -0.029  0.002 -0.004  0.003 -0.026\n",
       "14  0.034 -0.013  0.009 -0.010  0.037 -0.002  0.032  0.028  0.017  0.085"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3),\n",
    "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "document_topics.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmGZ-Xqjiuow",
    "outputId": "03350edc-ef57-42eb-91f6-07223fbf8ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #8:\n",
      "Dominant Topics (top 3): ['T6', 'T2', 'T8']\n",
      "Paper Summary:\n",
      "709 \n",
      "TIME-SEQUENTIAL SELF-ORGANIZATION OF HIERARCHICAL \n",
      "NEURAL NETWORKS \n",
      "Ronald H. Silverman \n",
      "Cornell University Medical College, New York, NY 10021 \n",
      "Andrew S. Noetzel \n",
      "Polytechnic University, Brooklyn, NY 11201 \n",
      "ABSTRACT \n",
      "Self-organization of multi-layered networks can be realized \n",
      "by time-sequential organization of successive neural layers. \n",
      "Lateral inhibition operating in the surround of firing cells in \n",
      "each layer provides for unsupervised capture of excitation \n",
      "patterns presented by the pre\n",
      "\n",
      "Document #150:\n",
      "Dominant Topics (top 3): ['T4', 'T7', 'T1']\n",
      "Paper Summary:\n",
      "519 \n",
      "A BACK-PROPAGATION ALGORrFHM \n",
      "WITH OFFIMAL USE OF HIDDEN UNITS \n",
      "Yves Chauvin \n",
      "Thomson-CSF, Inc \n",
      "(and Psychology Department, Stanford University) \n",
      "630, Hansen Way (Suite 250) \n",
      "Palo Alto, CA 94306 \n",
      "ABSTRACT \n",
      "This paper presents a variation of the back-propagation algo- \n",
      "rithm that makes optimal use of a network hidden units by de- \n",
      "creasing an \"nergy\" term written as a function of the squared \n",
      "activations of these hidden units. The algorithm can automati- \n",
      "cally find optimal or nearly optimal\n",
      "\n",
      "Document #200:\n",
      "Dominant Topics (top 3): ['T9', 'T8', 'T10']\n",
      "Paper Summary:\n",
      "Analog Circuits for Constrained Optimization 777 \n",
      "Analog Circuits for Constrained Optimization \n",
      "John C. Platt x \n",
      "Computer Science Department, 256-80 \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "This paper explores whether analog circuitry can adequately per- \n",
      "form constrained optimization. Constrained optimization circuits \n",
      "are designed using the differential multiplier method. These cir- \n",
      "cuits fulfill time-varying constraints correctly. Example circuits in- \n",
      "clude a quadr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_numbers = [8, 150, 200]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzKHwMWsjJtn"
   },
   "source": [
    "**Examine a sample of three different documents. Based on the information provided in the *document summary*, do the top 3 topics for each of these sampled documents make sense? Refer back to the list of top 20 words for each topic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qATFCnk9sLgM"
   },
   "source": [
    "Once I refered to the list, I saw that they do make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oyMyhYnjVZ3"
   },
   "source": [
    "## **5) Topic Modeling with Latent Dirichlet Allocation (LDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8EAAj7AsI2V",
    "outputId": "a09df1a4-1171-4dd2-a4f8-fbe63006d477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 2.4 s, total: 1min 32s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740,\n",
    "                                   alpha='auto', eta='auto', random_state=42,\n",
    "                                   iterations=500, num_topics=TOTAL_TOPICS,\n",
    "                                   passes=20, eval_every=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rymHHiiO00Bj"
   },
   "source": [
    "> #### **A) Print topics, topics = 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kc55oSR9sOW3",
    "outputId": "d198acd4-1ede-44f8-e468-4d0412acba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.010*\"node\" + 0.006*\"vector\" + 0.006*\"bit\" + 0.006*\"class\" + 0.005*\"threshold\" + 0.005*\"size\" + 0.005*\"let\" + 0.005*\"theorem\" + 0.004*\"memory\" + 0.004*\"linear\" + 0.004*\"neuron\" + 0.004*\"bound\" + 0.004*\"probability\" + 0.004*\"rule\" + 0.004*\"capacity\" + 0.004*\"layer\" + 0.003*\"code\" + 0.003*\"proof\" + 0.003*\"complexity\" + 0.003*\"polynomial\"\n",
      "\n",
      "Topic #2:\n",
      "0.025*\"image\" + 0.011*\"object\" + 0.009*\"feature\" + 0.008*\"visual\" + 0.008*\"motion\" + 0.006*\"pixel\" + 0.005*\"unit\" + 0.004*\"layer\" + 0.004*\"direction\" + 0.004*\"position\" + 0.004*\"location\" + 0.004*\"representation\" + 0.004*\"view\" + 0.004*\"recognition\" + 0.004*\"filter\" + 0.004*\"chip\" + 0.004*\"face\" + 0.004*\"field\" + 0.003*\"processing\" + 0.003*\"signal\"\n",
      "\n",
      "Topic #3:\n",
      "0.026*\"state\" + 0.011*\"control\" + 0.011*\"action\" + 0.007*\"step\" + 0.007*\"policy\" + 0.006*\"optimal\" + 0.005*\"reinforcement_learning\" + 0.005*\"task\" + 0.005*\"controller\" + 0.005*\"rate\" + 0.004*\"convergence\" + 0.004*\"environment\" + 0.003*\"robot\" + 0.003*\"goal\" + 0.003*\"stochastic\" + 0.003*\"dynamic\" + 0.003*\"reward\" + 0.003*\"update\" + 0.003*\"vector\" + 0.003*\"equation\"\n",
      "\n",
      "Topic #4:\n",
      "0.013*\"classifier\" + 0.011*\"class\" + 0.011*\"classification\" + 0.010*\"training\" + 0.009*\"pattern\" + 0.009*\"vector\" + 0.009*\"feature\" + 0.006*\"distance\" + 0.005*\"image\" + 0.005*\"training_set\" + 0.004*\"unit\" + 0.004*\"local\" + 0.004*\"linear\" + 0.004*\"rbf\" + 0.003*\"transformation\" + 0.003*\"region\" + 0.003*\"task\" + 0.003*\"map\" + 0.003*\"test\" + 0.003*\"cluster\"\n",
      "\n",
      "Topic #5:\n",
      "0.007*\"subject\" + 0.006*\"control\" + 0.006*\"movement\" + 0.005*\"motor\" + 0.005*\"target\" + 0.005*\"human\" + 0.004*\"representation\" + 0.004*\"word\" + 0.004*\"arm\" + 0.004*\"position\" + 0.004*\"change\" + 0.004*\"task\" + 0.004*\"trajectory\" + 0.004*\"memory\" + 0.004*\"component\" + 0.003*\"hand\" + 0.003*\"query\" + 0.003*\"similarity\" + 0.003*\"behavior\" + 0.003*\"training\"\n",
      "\n",
      "Topic #6:\n",
      "0.019*\"training\" + 0.007*\"prediction\" + 0.007*\"trained\" + 0.005*\"feature\" + 0.005*\"test\" + 0.005*\"task\" + 0.004*\"training_set\" + 0.004*\"hmm\" + 0.004*\"net\" + 0.004*\"mlp\" + 0.004*\"vector\" + 0.004*\"experiment\" + 0.004*\"unit\" + 0.004*\"architecture\" + 0.004*\"context\" + 0.003*\"recognition\" + 0.003*\"hidden_unit\" + 0.003*\"class\" + 0.003*\"table\" + 0.003*\"layer\"\n",
      "\n",
      "Topic #7:\n",
      "0.009*\"distribution\" + 0.006*\"probability\" + 0.006*\"variable\" + 0.006*\"estimate\" + 0.005*\"approximation\" + 0.005*\"prior\" + 0.005*\"gaussian\" + 0.005*\"sample\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"bayesian\" + 0.004*\"training\" + 0.004*\"vector\" + 0.003*\"class\" + 0.003*\"matrix\" + 0.003*\"log\" + 0.003*\"variance\" + 0.003*\"equation\" + 0.003*\"kernel\" + 0.003*\"estimation\"\n",
      "\n",
      "Topic #8:\n",
      "0.024*\"unit\" + 0.012*\"state\" + 0.010*\"pattern\" + 0.008*\"training\" + 0.008*\"hidden_unit\" + 0.007*\"word\" + 0.007*\"sequence\" + 0.006*\"net\" + 0.006*\"activation\" + 0.006*\"layer\" + 0.006*\"architecture\" + 0.005*\"speech\" + 0.005*\"task\" + 0.005*\"rule\" + 0.005*\"recurrent\" + 0.004*\"node\" + 0.004*\"representation\" + 0.004*\"connection\" + 0.004*\"recognition\" + 0.004*\"step\"\n",
      "\n",
      "Topic #9:\n",
      "0.008*\"equation\" + 0.007*\"dynamic\" + 0.006*\"matrix\" + 0.006*\"signal\" + 0.006*\"vector\" + 0.005*\"noise\" + 0.005*\"solution\" + 0.004*\"rule\" + 0.004*\"neuron\" + 0.004*\"correlation\" + 0.004*\"pattern\" + 0.004*\"linear\" + 0.004*\"source\" + 0.004*\"eq\" + 0.003*\"filter\" + 0.003*\"cell\" + 0.003*\"component\" + 0.003*\"property\" + 0.003*\"map\" + 0.003*\"fixed_point\"\n",
      "\n",
      "Topic #10:\n",
      "0.023*\"neuron\" + 0.016*\"cell\" + 0.009*\"response\" + 0.008*\"stimulus\" + 0.007*\"activity\" + 0.006*\"circuit\" + 0.006*\"pattern\" + 0.006*\"signal\" + 0.006*\"current\" + 0.005*\"spike\" + 0.005*\"synaptic\" + 0.005*\"neural\" + 0.004*\"voltage\" + 0.004*\"frequency\" + 0.004*\"connection\" + 0.004*\"firing\" + 0.004*\"effect\" + 0.004*\"synapsis\" + 0.003*\"et_al\" + 0.003*\"channel\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6rLFln_tFX6"
   },
   "source": [
    "**What is a key difference in the weights produced using LSI vs LDA?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpaUsrs6tJr9"
   },
   "source": [
    "LSI contains negative weights, and LDA is not. This makes LDA weights easier to interperate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujVDR9wU087K"
   },
   "source": [
    "> #### **B) Topic Coherenence Scores**\n",
    "\n",
    "Let's now get an idea of how our topic model is performing by computing the perplexity and the coherence scores ($C_v$ and UMass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WO_UYI7N185g",
    "outputId": "799fc73d-e489-4362-dcf3-b111965ce5e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.45605085319193267\n",
      "Avg. Coherence Score (UMass): -1.0688358952765649\n",
      "Model Perplexity: -7.796756402935002\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                                      texts=norm_corpus_bigrams,\n",
    "                                                      dictionary=dictionary,\n",
    "                                                      coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                                         texts=norm_corpus_bigrams,\n",
    "                                                         dictionary=dictionary,\n",
    "                                                         coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brXeqs3XrfYs"
   },
   "source": [
    "> #### **C) (On your own) Try out a Different LDA Model Implementation from MALLET**\n",
    "\n",
    "For the sake of time, I try out a different LDA model implementation from a library called MALLET (MAchine Learning for LanguagE Toolkit). It's worth exploring different pretrained models because different models will work better in different situations. Run through the provided code and report back on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zQSc9aBsD35",
    "outputId": "9d4e216f-b862-4eeb-aad9-d9523d3ec3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-03 01:20:42--  http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
      "Resolving mallet.cs.umass.edu (mallet.cs.umass.edu)... 128.119.246.70\n",
      "Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://mallet.cs.umass.edu/dist/mallet-2.0.8.zip [following]\n",
      "--2022-11-03 01:20:42--  https://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
      "Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16184794 (15M) [application/zip]\n",
      "Saving to: ‘mallet-2.0.8.zip’\n",
      "\n",
      "mallet-2.0.8.zip    100%[===================>]  15.43M  17.4MB/s    in 0.9s    \n",
      "\n",
      "2022-11-03 01:20:43 (17.4 MB/s) - ‘mallet-2.0.8.zip’ saved [16184794/16184794]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip            # download mallet zip file\n",
    "!unzip -q mallet-2.0.8.zip                                        # unzip compressed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ol54oiUJsOAl"
   },
   "outputs": [],
   "source": [
    "MALLET_PATH = 'mallet-2.0.8/bin/mallet'                                                               # set path to mallet models\n",
    "lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=bow_corpus,             # set lda model parameters\n",
    "                                              num_topics=TOTAL_TOPICS, id2word=dictionary,\n",
    "                                              iterations=500, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYGQzrcysXhF",
    "outputId": "ef6cc4a0-50d1-4522-b04c-5ba97211ad5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "['training', 'hidden_unit', 'unit', 'prediction', 'training_set', 'net', 'layer', 'trained', 'task', 'generalization', 'test', 'average', 'expert', 'noise', 'architecture', 'hidden_layer', 'target', 'back_propagation', 'size', 'bias']\n",
      "\n",
      "Topic #2:\n",
      "['class', 'word', 'classification', 'training', 'classifier', 'feature', 'recognition', 'speech', 'pattern', 'experiment', 'character', 'trained', 'test', 'hmm', 'database', 'context', 'vector', 'frame', 'probability', 'error_rate']\n",
      "\n",
      "Topic #3:\n",
      "['circuit', 'current', 'neuron', 'chip', 'signal', 'bit', 'analog', 'voltage', 'threshold', 'computation', 'implementation', 'element', 'neural', 'channel', 'design', 'code', 'node', 'processor', 'layer', 'operation']\n",
      "\n",
      "Topic #4:\n",
      "['image', 'object', 'visual', 'motion', 'feature', 'location', 'position', 'map', 'direction', 'field', 'pixel', 'target', 'region', 'view', 'face', 'local', 'human', 'subject', 'orientation', 'velocity']\n",
      "\n",
      "Topic #5:\n",
      "['state', 'control', 'action', 'step', 'task', 'policy', 'environment', 'controller', 'optimal', 'reinforcement_learning', 'goal', 'path', 'trajectory', 'robot', 'current', 'trial', 'move', 'cost', 'change', 'search']\n",
      "\n",
      "Topic #6:\n",
      "['neuron', 'cell', 'response', 'stimulus', 'activity', 'signal', 'pattern', 'spike', 'frequency', 'synaptic', 'effect', 'neural', 'cortical', 'et_al', 'firing', 'connection', 'layer', 'brain', 'cortex', 'temporal']\n",
      "\n",
      "Topic #7:\n",
      "['distribution', 'probability', 'estimate', 'approximation', 'variable', 'class', 'sample', 'prior', 'bound', 'theorem', 'log', 'density', 'bayesian', 'theory', 'gaussian', 'estimation', 'note', 'assume', 'give', 'xi']\n",
      "\n",
      "Topic #8:\n",
      "['vector', 'matrix', 'component', 'linear', 'constraint', 'solution', 'cluster', 'distance', 'local', 'transformation', 'structure', 'feature', 'gaussian', 'source', 'dimensional', 'clustering', 'technique', 'mapping', 'dimension', 'nonlinear']\n",
      "\n",
      "Topic #9:\n",
      "['equation', 'dynamic', 'state', 'vector', 'solution', 'rate', 'neuron', 'convergence', 'gradient', 'eq', 'matrix', 'pattern', 'nonlinear', 'energy', 'noise', 'rule', 'theory', 'capacity', 'attractor', 'continuous']\n",
      "\n",
      "Topic #10:\n",
      "['unit', 'node', 'rule', 'representation', 'structure', 'pattern', 'sequence', 'activation', 'level', 'memory', 'tree', 'connectionist', 'symbol', 'architecture', 'recurrent', 'string', 'context', 'language', 'instance', 'connection']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = [[(term, round(wt, 3))\n",
    "               for term, wt in lda_mallet.show_topic(n, topn=20)]                                     # save topics as list, round weights to 3 decimanls, top 20 words per topic\n",
    "                   for n in range(0, TOTAL_TOPICS)]\n",
    "\n",
    "for idx, topic in enumerate(topics):                                                                  # print each topic with their top 20 terms\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBpWmWXZt662",
    "outputId": "29e6c2e1-2e89-454f-ae5f-7ab10b58e7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.5084512527679875\n",
      "Avg. Coherence Score (UMass): -1.1026460248683745\n",
      "Model Perplexity: -8.53533\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda_mallet = gensim.models.CoherenceModel(model=lda_mallet, corpus=bow_corpus,     # set parameters of mallet lda model, c_v coherence score\n",
    "                                                             texts=norm_corpus_bigrams,\n",
    "                                                             dictionary=dictionary,\n",
    "                                                             coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda_mallet.get_coherence()                                      # produce average coherence scores (c_v) from mallet model\n",
    "\n",
    "umass_coherence_model_lda_mallet = gensim.models.CoherenceModel(model=lda_mallet, corpus=bow_corpus,  # set parameters of mallet lda mode, umass coherence score\n",
    "                                                                texts=norm_corpus_bigrams,\n",
    "                                                                dictionary=dictionary,\n",
    "                                                                coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda_mallet.get_coherence()                                # produce average coherence score (umass)\n",
    "\n",
    "# from STDOUT: <500> LL/token: -8.53533\n",
    "perplexity = -8.53533\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)                                                 # print perplexity, umass and C_v scores\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_StHlj0At-nH"
   },
   "source": [
    "**How do the metrics of topic model quality differ between the basic gensim implementation and the mallet implementation? Which model appeared to perform better in this case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1_VIvNPuN6M"
   },
   "source": [
    "I think the gensim may be better due to the UMass scores.\n",
    "\n",
    "The performance of the Mallet model appears to be better given the higher CV score. However, neither model is performing significantly well, and based on some additonal reading I did, CV scores aren't without issues. (especially when using gensim models.) I believe UMass is recommended more often)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ4_k23iuOqv"
   },
   "source": [
    "## **6) Parameter Tuning a Topic Model**\n",
    "\n",
    "Parameter tuning for an unsupervised text mining technique like topic modeling is odd since we don't have direct evaluation metrics. But, I use the topic coherence scores to try and find an optimal number of topics, whereby I find the fewest topics (to maximize simplicity) for the highest coherence score (to maximize topic quality). I examine this iteratively to determine the optimal number of topics; and produce a visualization to help make conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_I_p93MvQaC"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HMvk0BkvKbv"
   },
   "outputs": [],
   "source": [
    "def topic_model_coherence_generator(corpus, texts, dictionary,\n",
    "                                    start_topic_count=2, end_topic_count=10, step=1,                                      # define topic_model_coherence_generator function\n",
    "                                    cpus=1):\n",
    "\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=corpus,                       # for each number of topics iteratively fit mallet lda\n",
    "                                                            num_topics=topic_nums, id2word=dictionary,\n",
    "                                                            iterations=500, workers=cpus)\n",
    "        cv_coherence_model_mallet_lda = gensim.models.CoherenceModel(model=mallet_lda_model, corpus=corpus,               # for each number of topics iteratively compute c_v score\n",
    "                                                                     texts=texts, dictionary=dictionary,\n",
    "                                                                     coherence='c_v')\n",
    "        coherence_score = cv_coherence_model_mallet_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)                                                                          # create list of coherence scores for each iteration\n",
    "        models.append(mallet_lda_model)\n",
    "\n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-gFh7MwvZQx",
    "outputId": "1d991cc1-6149-41ad-aa9e-ee33b26072fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [34:40<00:00, 160.06s/it]\n"
     ]
    }
   ],
   "source": [
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus, texts=norm_corpus_bigrams,\n",
    "                                                               dictionary=dictionary, start_topic_count=6,\n",
    "                                                               end_topic_count=18, step=1, cpus=8)                    # change out the start_topic_count and end_topic_count values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jlJjfh0_UgfD",
    "outputId": "88d36ef6-887b-4f24-c727-29b88ff3bb36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics: 17\n",
      "cv score 0.5342268576971536\n"
     ]
    }
   ],
   "source": [
    "print(\"number of topics:\",6+np.argmax(coherence_scores))\n",
    "print(\"cv score\", max(coherence_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0s7jV1HV6Kk9"
   },
   "source": [
    "> #### **A) Evaluate a larger number of topics**\n",
    "\n",
    "> I went back and trained topic models on a large range of topics to try and find an optimal amount. Running this will take some time, and the larger the range the more time it will take to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEJEBnKt6rJ1"
   },
   "source": [
    "I ran it twice. First for 6-14 with a cv score of .53. I then went back and ran it a seocnd time 6-18, and got a cv score of .53. The best number of topics was 17, however, my cv score did not improve significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiyJFZ7-0S1X"
   },
   "source": [
    "> #### **B) Produce table of topic numbers by coherence scores; then plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PallcNEWz_Om",
    "outputId": "2454f303-0019-45da-f86f-a4636994cfa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.45605085319193267\n",
      "Avg. Coherence Score (UMass): -1.0688358952765649\n",
      "Model Perplexity: -7.796753901993755\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                                      texts=norm_corpus_bigrams,\n",
    "                                                      dictionary=dictionary,\n",
    "                                                      coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                                         texts=norm_corpus_bigrams,\n",
    "                                                         dictionary=dictionary,\n",
    "                                                         coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "JCYgkcYA3YPt",
    "outputId": "1776cb98-bcee-4965-c51e-9b6ccd6df741"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(6, 19)                                                                   # Adjust the range based on the number/range of topics you chose\n",
    "y_ax = coherence_scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "plt.axhline(y=0.525, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxAyPrUxVvoP",
    "outputId": "4efef106-c018-49a5-f72b-c129775b569e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4869242522980662,\n",
       " 0.5174551168364557,\n",
       " 0.5082903708785935,\n",
       " 0.48892049064452286,\n",
       " 0.5224630301931937,\n",
       " 0.5221592880164642,\n",
       " 0.521586159781423,\n",
       " 0.5063775210981714,\n",
       " 0.5295906434429605,\n",
       " 0.5240856074175767,\n",
       " 0.5311189209652656,\n",
       " 0.5342268576971536,\n",
       " 0.5223166208752573]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "n3a3Hu1BVv6w",
    "outputId": "922e6293-cd62-419c-cf93-792ddd2317e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cf328419-1631-4d85-bb2f-48fed3de8ce3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Coherence Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>0.5342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>0.5311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>0.5296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>0.5241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf328419-1631-4d85-bb2f-48fed3de8ce3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cf328419-1631-4d85-bb2f-48fed3de8ce3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cf328419-1631-4d85-bb2f-48fed3de8ce3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    Number of Topics  Coherence Score\n",
       "11                17           0.5342\n",
       "10                16           0.5311\n",
       "8                 14           0.5296\n",
       "9                 15           0.5241\n",
       "4                 10           0.5225"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_df = pd.DataFrame({'Number of Topics': range(6, 19),\n",
    "                             'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df.sort_values(by=['Coherence Score'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBCRgrWC3vQ7"
   },
   "source": [
    "> #### **C) Determine the Optimal Number of Topics and then rerun model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zIR5do63ufQ",
    "outputId": "fbdf3e6a-2316-4dbf-c133-4e3ba3661a3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_idx = coherence_df[coherence_df['Number of Topics'] == 10].index[0]\n",
    "best_lda_model = lda_models[best_model_idx]\n",
    "best_lda_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "VL8WdNSz3-65",
    "outputId": "adff6b9f-da8e-4173-df1c-51bb1318dedb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nfor idx, topic in enumerate(topics):\\n    print('Topic #'+str(idx+1)+':')\\n    print([term for term, wt in topic])\\n    print()\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = [[(term, round(wt, 3))\n",
    "               for term, wt in best_lda_model.show_topic(n, topn=20)]\n",
    "                   for n in range(0, best_lda_model.num_topics)]\n",
    "'''\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbr6bt9P4RiL"
   },
   "source": [
    "> #### **D) Display as Term-Topic Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "o3dfgJRm4V_7",
    "outputId": "56cb1876-5676-48a5-b791-61da3a4ff201"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-fe7bbcbf-4c25-492e-9a18-8f670d2b703e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>structure, vector, memory, pattern, representation, rule, node, cluster, code, graph, sequence, bit, level, distance, clustering, capacity, similarity, search, local, feature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>cell, neuron, response, stimulus, activity, pattern, effect, receptive_field, cortical, et_al, layer, unit, synaptic, connection, brain, cortex, firing, neural, mechanism, visual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>neuron, signal, circuit, current, chip, channel, analog, voltage, neural, noise, spike, frequency, delay, threshold, implementation, synapse, computation, gain, source, design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>unit, training, layer, net, hidden_unit, architecture, task, pattern, activation, trained, recurrent, node, rule, back_propagation, generalization, hidden_layer, connectionist, module, prediction, learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>distribution, estimate, gaussian, variable, probability, prior, sample, mixture, density, prediction, variance, estimation, approximation, bayesian, component, likelihood, noise, regression, log, procedure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>image, object, feature, motion, position, visual, direction, pixel, location, field, view, target, region, representation, filter, local, human, map, subject, surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>state, control, action, step, task, policy, environment, trajectory, optimal, controller, transition, reinforcement_learning, path, goal, dynamic, robot, current, change, sequence, trial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>class, bound, tree, size, theorem, probability, linear, complexity, threshold, node, theory, machine, loss, approximation, proof, hypothesis, polynomial, defined, distribution, assume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>training, feature, classification, word, recognition, classifier, class, speech, test, experiment, trained, face, character, pattern, table, context, database, vector, training_set, accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>equation, vector, matrix, solution, linear, dynamic, eq, rate, convergence, nonlinear, gradient, noise, optimal, theory, rule, line, find, source, energy, average</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe7bbcbf-4c25-492e-9a18-8f670d2b703e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-fe7bbcbf-4c25-492e-9a18-8f670d2b703e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-fe7bbcbf-4c25-492e-9a18-8f670d2b703e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                                                                                                                                                                       Terms per Topic\n",
       "Topic1   structure, vector, memory, pattern, representation, rule, node, cluster, code, graph, sequence, bit, level, distance, clustering, capacity, similarity, search, local, feature                               \n",
       "Topic2   cell, neuron, response, stimulus, activity, pattern, effect, receptive_field, cortical, et_al, layer, unit, synaptic, connection, brain, cortex, firing, neural, mechanism, visual                           \n",
       "Topic3   neuron, signal, circuit, current, chip, channel, analog, voltage, neural, noise, spike, frequency, delay, threshold, implementation, synapse, computation, gain, source, design                              \n",
       "Topic4   unit, training, layer, net, hidden_unit, architecture, task, pattern, activation, trained, recurrent, node, rule, back_propagation, generalization, hidden_layer, connectionist, module, prediction, learn   \n",
       "Topic5   distribution, estimate, gaussian, variable, probability, prior, sample, mixture, density, prediction, variance, estimation, approximation, bayesian, component, likelihood, noise, regression, log, procedure\n",
       "Topic6   image, object, feature, motion, position, visual, direction, pixel, location, field, view, target, region, representation, filter, local, human, map, subject, surface                                       \n",
       "Topic7   state, control, action, step, task, policy, environment, trajectory, optimal, controller, transition, reinforcement_learning, path, goal, dynamic, robot, current, change, sequence, trial                   \n",
       "Topic8   class, bound, tree, size, theorem, probability, linear, complexity, threshold, node, theory, machine, loss, approximation, proof, hypothesis, polynomial, defined, distribution, assume                      \n",
       "Topic9   training, feature, classification, word, recognition, classifier, class, speech, test, experiment, trained, face, character, pattern, table, context, database, vector, training_set, accuracy               \n",
       "Topic10  equation, vector, matrix, solution, linear, dynamic, eq, rate, convergence, nonlinear, gradient, noise, optimal, theory, rule, line, find, source, energy, average                                           "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])\n",
    "                              for topic in topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, best_lda_model.num_topics+1)]\n",
    "                         )\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIquI71R5NTj"
   },
   "source": [
    "> #### **E) Topic Model Interpretation**\n",
    "\n",
    "One meaningful way we can interpret our topic models is to see what the most dominant (highest weighted) topics were in specific research papers. Let's create a dataframe containing the document number, its most dominant topic number, the % contribution of that topic, the topic description and an excerpt from the beginning of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1xvsUaP50es"
   },
   "outputs": [],
   "source": [
    "tm_results = best_lda_model[bow_corpus]                                                           # ~ 21 seconds to run\n",
    "\n",
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0]\n",
    "                     for topics in tm_results]\n",
    "\n",
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(papers))                                               # create corpus topic df with specified columns (from instructions)\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "corpus_topic_df['Paper'] = papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "IeswCd_-6Aam",
    "outputId": "dc6496a1-9bf7-4a23-f1f4-586b8dc222f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-367984cd-19e6-4a04-a999-24434ef9b2a9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Topic Desc</th>\n",
       "      <th>Paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>36.92</td>\n",
       "      <td>image, object, feature, motion, position, visual, direction, pixel, location, field, view, target, region, representation, filter, local, human, map, subject, surface</td>\n",
       "      <td>622 \\nLEARNING A COLOR ALGORITHM FROM EXAMPLES \\nAnya C. Hurlbert and Tomaso A. Poggio \\nArtificial Intelligence Laboratory and Department of Brain and Cognitive Sciences, \\nMassachusetts Institut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>40.81</td>\n",
       "      <td>unit, training, layer, net, hidden_unit, architecture, task, pattern, activation, trained, recurrent, node, rule, back_propagation, generalization, hidden_layer, connectionist, module, prediction,...</td>\n",
       "      <td>442 \\nHow Neural Nets Work \\nAlan Lapedes \\nRobert Father \\nTheoretical Division \\nLos Alamos National Laboratory \\nLos Alamos, NM 87545 \\nAbstract: \\nThere is presently great interest in the abil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>42.26</td>\n",
       "      <td>structure, vector, memory, pattern, representation, rule, node, cluster, code, graph, sequence, bit, level, distance, clustering, capacity, similarity, search, local, feature</td>\n",
       "      <td>174 \\nA Neural Network C1A-sifier Based on Coding Theory \\nTzi-Dar Chiueh and Rodney Goodman \\nCalifornia Institute of Technology, Pasadena, California 91125 \\nABSTRACT\\nThe new neural network cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>392</td>\n",
       "      <td>10</td>\n",
       "      <td>24.31</td>\n",
       "      <td>equation, vector, matrix, solution, linear, dynamic, eq, rate, convergence, nonlinear, gradient, noise, optimal, theory, rule, line, find, source, energy, average</td>\n",
       "      <td>A Framework for the Cooperation \\nof Learning Algorithms \\nL6on Bottou \\nPatrick Gailinari \\nLaboratoire de Recherche en Informatique \\nUniversit6 de Paris XI \\n91405 Orsay Cedex \\nFrance \\nAbstra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>503</td>\n",
       "      <td>5</td>\n",
       "      <td>46.67</td>\n",
       "      <td>distribution, estimate, gaussian, variable, probability, prior, sample, mixture, density, prediction, variance, estimation, approximation, bayesian, component, likelihood, noise, regression, log, ...</td>\n",
       "      <td>Kernel Regression and \\nBackpropagation Training with Noise \\nPetri Koistlnen and Lasse HolmstrSm \\nRolf Nevanlinna Institute, University of Helsinki \\nTeollisuuskatu 23, SF-00510 Helsinki, Finlan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>682</td>\n",
       "      <td>3</td>\n",
       "      <td>38.38</td>\n",
       "      <td>neuron, signal, circuit, current, chip, channel, analog, voltage, neural, noise, spike, frequency, delay, threshold, implementation, synapse, computation, gain, source, design</td>\n",
       "      <td>Analog VLSI Implementation of \\nMulti-dimensional Gradient Descent \\nDavid B. Kirk Douglas Kerns Kurt Fleischer Alan H. Barr \\nCalifornia Institute of Technology \\nBeckman Institute 350-74 \\nPa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>736</td>\n",
       "      <td>10</td>\n",
       "      <td>41.36</td>\n",
       "      <td>equation, vector, matrix, solution, linear, dynamic, eq, rate, convergence, nonlinear, gradient, noise, optimal, theory, rule, line, find, source, energy, average</td>\n",
       "      <td>Non-linear Statistical Analysis and \\nSelf-Organizing Hebbian Networks \\nJonathan L. Shapiro and Adam Priigel-Bennett \\nDepartment of Computer Science \\nThe University, Manchester \\nManchester, UK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>906</td>\n",
       "      <td>4</td>\n",
       "      <td>53.23</td>\n",
       "      <td>unit, training, layer, net, hidden_unit, architecture, task, pattern, activation, trained, recurrent, node, rule, back_propagation, generalization, hidden_layer, connectionist, module, prediction,...</td>\n",
       "      <td>Learning Many Related Tasks at the \\nSame Time With Backpropagation \\nRich Caruana \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213 \\ncaruana@ cs. cmu .edu \\nAbstra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1005</td>\n",
       "      <td>8</td>\n",
       "      <td>36.65</td>\n",
       "      <td>class, bound, tree, size, theorem, probability, linear, complexity, threshold, node, theory, machine, loss, approximation, proof, hypothesis, polynomial, defined, distribution, assume</td>\n",
       "      <td>Using Pairs of Data-Points to Define \\nSplits for Decision Trees \\nGeoffrey E. Hinton \\nDepartment of Computer Science \\nUniversity of Toronto \\nToronto, Ontario, M5S 1A4, Canada \\nhinton@cs.toron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>1622</td>\n",
       "      <td>3</td>\n",
       "      <td>38.29</td>\n",
       "      <td>neuron, signal, circuit, current, chip, channel, analog, voltage, neural, noise, spike, frequency, delay, threshold, implementation, synapse, computation, gain, source, design</td>\n",
       "      <td>Information Capacity and Robustness of \\nStochastic Neuron Models \\nElad Schneidman Idan Segev Naftali Tishby \\nInstitute of Computer Science, \\nDepartment of Neurobiology and \\nCenter for Neural ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-367984cd-19e6-4a04-a999-24434ef9b2a9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-367984cd-19e6-4a04-a999-24434ef9b2a9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-367984cd-19e6-4a04-a999-24434ef9b2a9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Document  Dominant Topic  Contribution %  \\\n",
       "10          10               6           36.92   \n",
       "13          13               4           40.81   \n",
       "17          17               1           42.26   \n",
       "392        392              10           24.31   \n",
       "503        503               5           46.67   \n",
       "682        682               3           38.38   \n",
       "736        736              10           41.36   \n",
       "906        906               4           53.23   \n",
       "1005      1005               8           36.65   \n",
       "1622      1622               3           38.29   \n",
       "\n",
       "                                                                                                                                                                                                   Topic Desc  \\\n",
       "10                                     image, object, feature, motion, position, visual, direction, pixel, location, field, view, target, region, representation, filter, local, human, map, subject, surface   \n",
       "13    unit, training, layer, net, hidden_unit, architecture, task, pattern, activation, trained, recurrent, node, rule, back_propagation, generalization, hidden_layer, connectionist, module, prediction,...   \n",
       "17                             structure, vector, memory, pattern, representation, rule, node, cluster, code, graph, sequence, bit, level, distance, clustering, capacity, similarity, search, local, feature   \n",
       "392                                        equation, vector, matrix, solution, linear, dynamic, eq, rate, convergence, nonlinear, gradient, noise, optimal, theory, rule, line, find, source, energy, average   \n",
       "503   distribution, estimate, gaussian, variable, probability, prior, sample, mixture, density, prediction, variance, estimation, approximation, bayesian, component, likelihood, noise, regression, log, ...   \n",
       "682                           neuron, signal, circuit, current, chip, channel, analog, voltage, neural, noise, spike, frequency, delay, threshold, implementation, synapse, computation, gain, source, design   \n",
       "736                                        equation, vector, matrix, solution, linear, dynamic, eq, rate, convergence, nonlinear, gradient, noise, optimal, theory, rule, line, find, source, energy, average   \n",
       "906   unit, training, layer, net, hidden_unit, architecture, task, pattern, activation, trained, recurrent, node, rule, back_propagation, generalization, hidden_layer, connectionist, module, prediction,...   \n",
       "1005                  class, bound, tree, size, theorem, probability, linear, complexity, threshold, node, theory, machine, loss, approximation, proof, hypothesis, polynomial, defined, distribution, assume   \n",
       "1622                          neuron, signal, circuit, current, chip, channel, analog, voltage, neural, noise, spike, frequency, delay, threshold, implementation, synapse, computation, gain, source, design   \n",
       "\n",
       "                                                                                                                                                                                                        Paper  \n",
       "10    622 \\nLEARNING A COLOR ALGORITHM FROM EXAMPLES \\nAnya C. Hurlbert and Tomaso A. Poggio \\nArtificial Intelligence Laboratory and Department of Brain and Cognitive Sciences, \\nMassachusetts Institut...  \n",
       "13    442 \\nHow Neural Nets Work \\nAlan Lapedes \\nRobert Father \\nTheoretical Division \\nLos Alamos National Laboratory \\nLos Alamos, NM 87545 \\nAbstract: \\nThere is presently great interest in the abil...  \n",
       "17    174 \\nA Neural Network C1A-sifier Based on Coding Theory \\nTzi-Dar Chiueh and Rodney Goodman \\nCalifornia Institute of Technology, Pasadena, California 91125 \\nABSTRACT\\nThe new neural network cla...  \n",
       "392   A Framework for the Cooperation \\nof Learning Algorithms \\nL6on Bottou \\nPatrick Gailinari \\nLaboratoire de Recherche en Informatique \\nUniversit6 de Paris XI \\n91405 Orsay Cedex \\nFrance \\nAbstra...  \n",
       "503   Kernel Regression and \\nBackpropagation Training with Noise \\nPetri Koistlnen and Lasse HolmstrSm \\nRolf Nevanlinna Institute, University of Helsinki \\nTeollisuuskatu 23, SF-00510 Helsinki, Finlan...  \n",
       "682   Analog VLSI Implementation of \\nMulti-dimensional Gradient Descent \\nDavid B. Kirk Douglas Kerns Kurt Fleischer Alan H. Barr \\nCalifornia Institute of Technology \\nBeckman Institute 350-74 \\nPa...  \n",
       "736   Non-linear Statistical Analysis and \\nSelf-Organizing Hebbian Networks \\nJonathan L. Shapiro and Adam Priigel-Bennett \\nDepartment of Computer Science \\nThe University, Manchester \\nManchester, UK...  \n",
       "906   Learning Many Related Tasks at the \\nSame Time With Backpropagation \\nRich Caruana \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213 \\ncaruana@ cs. cmu .edu \\nAbstra...  \n",
       "1005  Using Pairs of Data-Points to Define \\nSplits for Decision Trees \\nGeoffrey E. Hinton \\nDepartment of Computer Science \\nUniversity of Toronto \\nToronto, Ontario, M5S 1A4, Canada \\nhinton@cs.toron...  \n",
       "1622  Information Capacity and Robustness of \\nStochastic Neuron Models \\nElad Schneidman Idan Segev Naftali Tishby \\nInstitute of Computer Science, \\nDepartment of Neurobiology and \\nCenter for Neural ...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "(corpus_topic_df[corpus_topic_df['Document']\n",
    "                 .isin([682, 10, 392, 1622, 17,\n",
    "                        906, 1005, 503, 13, 736])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvX0DRM56z4v"
   },
   "source": [
    "**Examine several of the papers displayed in the dataframe. What characterizes the dominant topics for these papers? Do they reasonably match up with the paper title?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwaKMusQ7Sq_"
   },
   "source": [
    "Most of these papers had to do with Machine Learning, and more specifically, neural networks.\n",
    "\n",
    "I would say so. If look at the topics description, I actually have a pretty good idea of what the papers are about (in general). I may not get specifics but I do get the gist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-iP66em7S5W"
   },
   "source": [
    "**What is another context, other than trying to briefly summarize the overall themes/topics, where topic modeling could be useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guE5Sz857eCZ"
   },
   "source": [
    "Topic modeling can be useful when analyzing reviews or customer feedback forms. It can also be useful when a company or individual doesn't have time to read through documents or articles. Once a model is trained they provide a template for directly and indirectly reading inputs for further analysis. This significantly reduces the time a person would have to take to read through a match up of documents. Some applications of this include modeling emails, contracts, accounting documents, etc.\n",
    "\n",
    "One idea I had in regards to applications of topic modeling is to take laws (Since that text data would be easily accessable) for some topic (for example, gun control) across 50 states, and see how similar certain laws on the same topic are written. Then compare that to perhaps crime statistics for that state. Is there any correlation between certain provisions written in our laws and crime/violence?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
